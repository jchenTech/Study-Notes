  

## 1 常微分方程的离散化

考虑一阶常微分方程的初值问题
$$
\left\{ {\begin{array}{*{20}{l}}
{\frac{{dy}}{{dx}} = f(x,y)\quad x \in [a,b]}\\
{y(a) = {y_0}}
\end{array}} \right.
$$
只要 $f(x, y)$ 在 $[a, b] \times R^{1}$ 上连续，且关于 $\boldsymbol{y}$ 满足 Lipschitz 条件，即存在与 $x, y$ 无关的常数 $L$ 使 $\left|f\left(x, y_{1}\right)-f\left(x, y_{2}\right)\right| \leq L\left|y_{1}-y_{2}\right|$ 对任意定义在 $[a, b]$ 上的 $y_{1}(x)$ 和$y_{2}(x)$ 都成立，则上述IVP存在唯一解。

本章的任务：计算出解函数 $y(x)$ 在一系列节点 $a=x_{0}<x_{1}<\ldots<x_{n}=b \quad y_{n} \approx y\left(x_{n}\right) \quad(n=1, \ldots, N)$ 处的近似值。



微分方程数值解法，其实是求出方程的解 $y(x)$ 在一系列离散点上的近似值。则微分方程数值解的基本思想是：求解区间和方程离散化。

### 1.1 求解区间离散化

将求解区间 $[a, b]$ 离散化, 是在 $[a, b]$ 上插入一系列的分 点 $\left\{x_{k}\right\},$ 使 $a=x_{0}<x_{1}<\ldots<x_{n}<x_{n+1}<\ldots<x_{N}=b$记 $h_{n}=x_{n+1}-x_{n}(n=0,1, \ldots, N-1)$ 称为步长，一般取 $h_{n}=h$(常数),节点为 $x_{n}=x_{0}+n h \quad(n=0,1,2, \ldots, N), h=\frac{b-a}{N}$称为等步长节点 。

### 1.2 微分方程离散化

1. 差商逼近法
   即是用适当的差商逼近导数值。

2. 数值积分法
   $$
   y({x_m}) - y({x_n}) = \int_{{x_n}}^{{x_m}} {f(x,y(x))dx}
   $$
   然后将上式右端采用数值积分离散化，从而获得原初值问题的一个离散差分格式。

3. Taylor展开法

## 2 欧拉方法

### 2.1 显式欧拉

用**向前差商近似导数**：$y'({x_0}) \approx \frac{{y({x_1}) - y({x_0})}}{h}$ 

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119201956.png" alt="image-20201119201956714" style="zoom: 50%;" />

可以理解为用 $x_0$ 到 $x_1$ 的斜率来近似 $x_0$ 的导数。因此
$$
y({x_1}) \approx y({x_0}) + hy'({x_0}) = {y_0} + hf({x_0},{y_0})
$$
可以表示为：
$$
{y_{n + 1}} = {y_n} + hf({x_n},{y_n})\quad (n = 0,...,N - 1)
$$
<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119202541.png" alt="image-20201119202541414" style="zoom:50%;" />

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119202719.png" alt="image-20201119202718930" style="zoom: 25%;" />

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119202737.png" alt="image-20201119202737697" style="zoom: 25%;" />

从图中可以看出，灰色连续曲线为处置问题的解析解，红色点为数值解，发现随着x的增大，数值解与曲线偏差越来越大，因此显示**欧拉方法的误差是很大的**。

### 2.2 隐式欧拉法

**向后差商近似导数**：$y'({x_1}) \approx \frac{{y({x_1}) - y({x_0})}}{h}$ 

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119203043.png" alt="image-20201119203043541" style="zoom:50%;" />

可以理解为用 $x_0$ 到 $x_1$ 的斜率来近似 $x_1$ 的导数。因此
$$
y\left(x_{1}\right) \approx y_{0}+h f\left(x_{1}, y\left(x_{1}\right)\right)
$$
可以表示为：
$$
y_{n+1}=y_{n}+h f\left(x_{n+1}, y_{n+1}\right) \quad(n=0, \ldots, N-1)
$$
由于未知数 $y_{n+1}$ 同时出现在等式的两边，不能直接得到，因此一般先用显式计算一个初值，再迭代求解。
$$
\left\{ \begin{array}{l}
y_{n + 1}^{(0)} = {y_n} + hf({x_n},{y_n})\\
y_{n + 1}^{(k + 1)} = {y_n} + hf({x_{n + 1}},y_{n + 1}^{(k)})\quad k=0,1,2...
\end{array} \right.
$$
如果迭代过程收敛，则某步后 $y_{n+1}^{(k)}$ 就可以作为 $y_{n+1},$ 从而进行 下一步的计算。

### 2.3 梯形公式——显、隐式两种算法的平均

$$
{y_{n + 1}} = {y_n} + \frac{h}{2}[f({x_n},{y_n}) + f({x_{n + 1}},{y_{n + 1}})]\quad (n = 0,\;...\;,N - 1)
$$

### 2.4 两步欧拉公式

**中心差商近似导数**： $y'({x_1}) \approx \frac{{y({x_2}) - y({x_0})}}{{2h}}$ 

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119204914.png" alt="image-20201119204914306" style="zoom:50%;" />

可以理解为用 $x_0$ 到 $x_2$ 的斜率的一半来近似 $x_1$ 的导数。因此
$$
y({x_2}) \approx y({x_0}) + 2h\,f({x_1},y({x_1}))
$$
可以表示为：
$$
{y_{n + 1}} = {y_{n - 1}} + 2hf({x_n},{y_n})\quad n = 1,...,N - 1
$$
需要两个点 $y_n$ 和 $y_{n-1}$  作为启动点

### 2.5 改进欧拉法

步骤为：

Step 1: 先用**显式欧拉**公式作预测，算出 $\bar{y}_{n+1}=y_{n}+h f\left(x_{n}, y_{n}\right)$ 

Step 2: 再将 $\bar{y}_{n+1}$ 代入**隐式梯形**公式的右边作校正，得到：
$$
y_{n+1}=y_{n}+\frac{h}{2}\left[f\left(x_{n}, y_{n}\right)+f\left(x_{n+1}, \bar{y}_{n+1}\right)\right]
$$

$$
{y_{n + 1}} = {y_n} + \frac{h}{2}\left[ {f({x_n},{y_n}) + f\left( {{x_{n + 1}},{y_n} + hf({x_n},{y_n})} \right)} \right]\quad (n = 0,...,N - 1)
$$

最终表示为：
$$
\begin{array}{l}
{y_p} = {y_n} + hf({x_n},{y_n})\;\\
{y_c} = {y_n} + hf({x_{n + 1}},{y_p})\\
{y_{n + 1}} = \frac{1}{2}({y_p} + {y_c})
\end{array}
$$
此法亦称为预测-校正法，可以证明该算法具有 2 阶精度，比显示欧拉稳定性高，比隐式欧拉迭代过程简单，比梯形公式为单步递推格式。

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119213351.png" alt="image-20201119213351871" style="zoom:25%;" />

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119213408.png" alt="image-20201119213408411" style="zoom: 33%;" />

从表和图可以看出，改进的Euler法的精度提高了不少。

### 2.6 局部截断误差

> 定义：在假设 $y_n = y(x_n)$ 情况下，即第n 步计算是精确的前提下，考虑的截断误差 $T_{n+1}= y(x_{n+1}) - y_{n+1} $ 称为**局部截断误差**

> 定义：若某算法的局部截断误差为O(hp+1)，则称该算法有p 阶精度。

一元函数在点 $x_{k}$ 处的泰勒展开式为：

$$
f(x)=f\left(x_{k}\right)+\left(x-x_{k}\right) f^{\prime}\left(x_{k}\right)+\frac{1}{2 !}\left(x-x_{k}\right)^{2} f^{\prime \prime}\left(x_{k}\right)+o^{n}
$$
二元函数在点 $\left(x_{k}, y_{k}\right)$ 处的泰勒展开式为：
$$
\begin{array}{c}f(x, y)=f\left(x_{k}, y_{k}\right)+\left(x-x_{k}\right) f_{x}^{\prime}\left(x_{k}, y_{k}\right)+\left(y-y_{k}\right) f_{y}^{\prime}\left(x_{k}, y_{k}\right) \\+\frac{1}{2 !}\left(x-x_{k}\right)^{2} f_{x x}^{\prime \prime}\left(x_{k}, y_{k}\right)+\frac{1}{2 !}\left(x-x_{k}\right)\left(y-y_{k}\right) f_{x y}^{\prime \prime}\left(x_{k}, y_{k}\right) \\+\frac{1}{2 !}\left(x-x_{k}\right)\left(y-y_{k}\right) f_{y x}^{\prime \prime}\left(x_{k}, y_{k}\right)+\frac{1}{2 !}\left(y-y_{k}\right)^{2} f_{y y}^{\prime \prime}\left(x_{k}, y_{k}\right) \\+o^{n}\end{array}
$$

**求局部截断误差的步骤** :

1) 作局部化假设，即设 $y_{n}=y(x_{n})$;

2) 将差分解 $y_{n+1}$ 在 $x_n$ 处展开

3) 将准确解 $y(x_{n+1})$ 在 $x_n$ 处做泰勒展开;

4) 比较 $T_{n+1}=y\left(x_{n+1}\right)-y_{n+1}$



欧拉法的局部截断误差：
$$
{T_{n + 1}} = y({x_{n + 1}}) - {y_{n + 1}} = [y({x_n}) + hy'({x_n}) + {\textstyle{{{h^2}} \over 2}}y''({x_n}) + O({h^3})] - [{y_n} + hf({x_n},{y_n})]\\
= {\textstyle{{{h^2}} \over 2}}y''({x_n}) + O({h^3})
$$
因此欧拉法具有1阶精度

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119221657.png" alt="img" style="zoom: 50%;" />

## 3 龙格—库塔法

**建立高精度的单步递推格式。**

基于改进的欧拉法可以写成：
$$
\begin{array}{*{20}{l}}
{{y_{n + 1}}}&{ = \quad {y_n} + h\left[ {\frac{1}{2}{K_1} + \frac{1}{2}{K_2}} \right]}\\
{{K_1}}&{ = \quad f({x_n},{y_n})}\\
{{K_2}}&{ = \quad f({x_n} + h,\,{y_n} + h{K_1})}
\end{array}
$$
显然， $k_1$ , $k_2$ 是在点 $x_n$, $x_{n+1}$ 处的斜率，但是斜率一定取平均值吗？而步长一定是一个h吗？在区间内多取几个点将他们的斜率加权平均作为平均斜率这样可能能构造出精度更高的计算公式，这就是龙格库塔的思想。

基于这点，将改进欧拉法推广为：
$$
\begin{array}{*{20}{l}}
{{y_{n + 1}}}&{ = \quad {y_n} + h\left[ {\lambda_1{K_1} + \lambda_2{K_2}} \right]} \quad\lambda_1+\lambda_2=1\\
{{K_1}}&{ = \quad f({x_n},{y_n})}\\
{{K_2}}&{ = \quad f({x_n} + ph,{y_n} + ph{K_1})} \quad 0 < p \le 1
\end{array}
$$
首先希望能确定系数 $\lambda_{1} 、 \lambda_{2} 、 p,$ 使得到的算法格式有2阶 精度，即在 $y_{n}=y\left(x_{n}\right)$ 的前提假设下，使得
$$
{T_{n + 1}} = y({x_{n + 1}}) - {y_{n + 1}} = O({h^3})
$$
具体推导为：

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201119224441.png" alt="image-20201119224440882" style="zoom: 33%;" />

### 3.1 龙格——库塔法的一般形式

$$
\begin{array}{*{20}{l}}
y_{n+1}=y_{n}+h \sum_{i=1}^{L} \lambda_{i} k_{i}\\
k_{1}=f\left(x_{n}, y_{n}\right)\\
k_{2}=f\left(x_{n}+c_{2} h, y_{n}+c_{2} h k_{1}\right)\\
k_{3}=f\left(x_{n}+c_{3} h, y_{n}+c_{3} h\left(a_{31} k_{1}+a_{32} k_{2}\right)\right)\\
\quad \cdots \cdots \cdots \cdots \cdots\\
k_{i}=f\left(x_{n}+c_{i} h, y_{n}+c_{i} h \sum_{j=1}^{i-1} a_{i j} k_{j}\right) \quad i=2,3, \cdots, L\\
\end{array}
$$

其中 $\sum_{i=1}^{L} \lambda_{i}=1, c_{i} \leq 1, \sum_{j=1}^{i-1} a_{i j}=1$ 均为待定系数，确定这些系数的步骤与前面相似。



最常用为四级**4阶经典龙格-库塔法**，简称RK方法。
$$
\begin{array}{*{20}{l}}
{{y_{n + 1}}}& = &{{y_n} + {\textstyle{h \over 6}}({K_1} + 2{K_2} + 2{K_3} + {K_4})}\\
{{K_1}}& = &{f({x_n},{y_n})}\\
{{K_2}}& = &{f({x_n} + {\textstyle{h \over 2}},{y_n} + {\textstyle{h \over 2}}{K_1})}\\
{{K_3}}& = &{f({x_n} + {\textstyle{h \over 2}},{y_n} + {\textstyle{h \over 2}}{K_2})}\\
{{K_4}}& = &{f({x_n} + h,{y_n} + h{K_3})}
\end{array}
$$





## 4 收敛性与稳定性

### 4.1 收敛性

1. 判断显式单步格式的收敛性，归结为验证增量函数 $\varphi$ 是否满足Lipschitz 条件。
2. 单步格式的整体截断误差由初值误差及局部截断误差决定，整体截断误差比局部截断误差的阶数**低一阶**。
3. 要构造高精度的计算方法，只需设法**提高局部截断误差的阶**即可。

### 4.2 稳定性

> 定义：若某算法在计算过程中任一步产生的误差在以后的计算中都逐步衰减，则称该算法是绝对稳定的

当步长取为 $h$ 时，并假设只在初值产生误差 $\varepsilon_{0}=y_{0}-\bar{y}_{0},$ 则若此误差以后逐步衰减，就称该 算法相对于 $\bar{h}=\lambda h$ 绝对稳定， $\bar{h}$ 的全体构成绝对稳定区域。 我们称算法 A 比算法B 稳定，就是指 A 的绝对稳定区域比 B 的大。

<img src="https://gitee.com/jchenTech/images/raw/master/img/20201120115743.png" alt="image-20201120115743250" style="zoom: 33%;" />

